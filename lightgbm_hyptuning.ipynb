{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "dataframe = pd.read_csv(r'C:\\Users\\User\\OneDrive\\Desktop\\predicting-brain-age\\Data\\original.csv')\n",
    "fe = pd.read_csv(r'C:\\Users\\User\\OneDrive\\Desktop\\predicting-brain-age\\Data\\binned_on_age_original_quartile32.csv')\n",
    "\n",
    "dataframe = pd.concat([dataframe,fe],axis = 1)\n",
    "\n",
    "dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features= dataframe.filter(regex = 'interaction').columns.tolist()\n",
    "\n",
    "X = dataframe.drop('age', axis=1)\n",
    "y = (dataframe['age'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Create a BinaryEncoder instance\n",
    "encoder = BinaryEncoder(cols=cat_features)\n",
    "\n",
    "# Fit and transform the data\n",
    "x_train_encoded = encoder.fit_transform(x_train[cat_features])\n",
    "x_test_encoded = encoder.transform(x_test[cat_features])\n",
    "# Output the transformed data\n",
    "\n",
    "x_train = x_train.drop(cat_features,axis = 1)\n",
    "x_train = pd.concat([x_train,x_train_encoded],axis = 1)\n",
    "\n",
    "x_test = x_test.drop(cat_features,axis = 1)\n",
    "x_test = pd.concat([x_test,x_test_encoded],axis = 1)\n",
    "\n",
    "x_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "space  = [\n",
    "    Integer(-1, 10, name='max_depth'),\n",
    "    # Integer(50, 500, name='n_estimators'),\n",
    "   \n",
    "    Real(0.5,1,name = 'feature_fraction',prior='log-uniform'),\n",
    "    Integer(50, 500, name='n_estimators'),\n",
    "\n",
    "    Integer(20, 200, name='min_data_in_leaf'),\n",
    "    Real(0.0001, 1, name='learning_rate',prior='log-uniform'),\n",
    "    Integer(2, 200, name='num_leaves'),\n",
    "    Real(0.1, 1.0, name='lambda_l1', prior='log-uniform'),  # L1 regularization\n",
    "    Real(0.1, 1.0, name='wlambda_l2', prior='log-uniform'),  # L2 regularization\n",
    "    Categorical(['gbdt', 'dart'], name='boosting_type')\n",
    "]\n",
    "\n",
    "# Initialize the model\n",
    "lgbm = LGBMRegressor(objective= 'regression',random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from skopt import gp_minimize\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    lgbm.set_params(**params)\n",
    "    lgbm.fit(x_train, y_train)\n",
    "    y_pred = lgbm.predict(x_test)\n",
    "    mse = mean_absolute_error((y_test), (y_pred))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gp_minimize(objective, space, n_calls=50, random_state=42)\n",
    "\n",
    "print(\"Best parameters: {}\".format(result.x))\n",
    "print(\"Best MAE: {:.5f}\".format(result.fun))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,r2_score\n",
    "from  scipy.stats import pearsonr\n",
    "\n",
    "lgbm.set_params(**dict(zip([dim.name for dim in space], result.x)))\n",
    "lgbm.fit(x_train, y_train)\n",
    "y_pred = lgbm.predict(x_test)\n",
    "final_mse = mean_absolute_error((y_test), (y_pred))\n",
    "print(f\"Final MAE: {final_mse}\")\n",
    "print('lightgbm r2 :',(r2_score((y_test),(y_pred))))\n",
    "print('lightgbm pearson :',(pearsonr((y_test),(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv = cross_val_score(lgbm,x_train,y_train,scoring='neg_mean_absolute_error',cv=10)\n",
    "print(np.mean(-cv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyternb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
